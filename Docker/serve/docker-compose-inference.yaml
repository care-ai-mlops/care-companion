name: inference_service
services:
  triton_server:
    build:
      context: /home/cc/care-companion/Docker
      dockerfile: serve/Dockerfile.triton
    container_name: triton_server
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ports:
      - "8000:8000"  # for HTTP requests
      - "8001:8001"  # for GRPC requests
      - "8002:8002"  # for reporting metrics


  fastapi:
    build:
      context: /home/cc/care-companion/Docker
      dockerfile: serve/Dockerfile.fastapi
    container_name: fastapi_server
    ports:
      - "80:5000"
    environment:
      - TRITON_SERVER_URL=triton_server:8000
